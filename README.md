ğŸ–¼ï¸ Image Classifier using Lightweight Vision Transformers

  A complete deep learning project for image classification, built using DeiT-Tiny and ViT-Tiny Vision Transformers.

The project includes full training scripts, dataset preprocessing tools, inference notebooks, a Flask-based web application, and four trained model weights:

  1. DeiT-Tiny (trained on 600-image custom dataset)
  2. DeiT-Tiny (trained on pooled dataset)
  3. ViT-Tiny (trained on 600-image custom dataset)
  4. ViT-Tiny (trained on pooled dataset)
     
This repository is designed to showcase how dataset size, diversity, and model architecture affect performance and generalization.


ğŸ“ Repository Structure
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ images/                  # 600-image custom dataset
â”‚   â”œâ”€â”€ pooled_images/           # Larger pooled dataset
â”‚   â”œâ”€â”€ labels.csv               # Annotations for custom dataset
â”‚   â”œâ”€â”€ processed_dataset.csv    # Cleaned dataset
â”‚   â”œâ”€â”€ attributes.yaml          # Attribute schema
â”‚   â”œâ”€â”€ classes.txt              # Class labels (10 classes)
â”‚   â””â”€â”€ ...  
â”‚
â”œâ”€â”€ other_codes/
â”‚   â”œâ”€â”€ clean_attributes.py
â”‚   â”œâ”€â”€ remove_synonyms.py
â”‚   â””â”€â”€ split.py                 # Train/val split utility
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ dataset.py               # Custom dataset loader
â”‚   â”œâ”€â”€ dataset_pooled.py        # Loader for pooled dataset
â”‚   â”œâ”€â”€ model.py                 # DeiT-Tiny & ViT-Tiny architectures
â”‚   â”œâ”€â”€ model_pooled.py
â”‚   â”œâ”€â”€ retrieval.py             # Cosine similarity retrieval
â”‚   â”œâ”€â”€ train_deit_tiny.py
â”‚   â”œâ”€â”€ train_deit_tiny_pooled.py
â”‚   â”œâ”€â”€ train_vit_tiny.py
â”‚   â””â”€â”€ train_vit_tiny_pooled.py
â”‚
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ best_model.pth           # DeiT-Tiny (custom dataset)
â”‚
â”œâ”€â”€ outputs_pooled/
â”‚   â”œâ”€â”€ best_model.pth           # DeiT-Tiny (pooled dataset)
â”‚
â”œâ”€â”€ outputs_vit_tiny/
â”‚   â”œâ”€â”€ best_model.pth           # ViT-Tiny (custom dataset)
â”‚
â”œâ”€â”€ outputs_vit_tiny_pooled/
â”‚   â”œâ”€â”€ best_model.pth           # ViT-Tiny (pooled dataset)
â”‚
â”œâ”€â”€ web_app/
â”‚   â”œâ”€â”€ app.py                   # Flask web application
â”‚   â”œâ”€â”€ templates/               # HTML templates
â”‚   â””â”€â”€ uploads/                 # Uploaded images during inference
â”‚
â”œâ”€â”€ inference_notebooks/
â”‚   â”œâ”€â”€ deit_tiny_inference.ipynb
â”‚   â””â”€â”€ vit_tiny_inference.ipynb
â”‚
â””â”€â”€ README.md


ğŸ“¦ Features

âœ” Two Lightweight Vision Transformers : 
  1. DeiT-Tiny (distilled: stable & efficient)
  2. ViT-Tiny (pure transformer baseline)
     
âœ” Dual-Dataset Training :
  Small 600-image custom dataset
  Large pooled dataset for generalization comparison

âœ” Complete Training Pipeline :
  1. Dataset loading
  2. Augmentation
  3. Fine-tuning
  4. Validation
  5. Saving best checkpoint

âœ” Four Trained Weights Included :
  1. DeiT-Tiny â€” Custom Dataset
  2. DeiT-Tiny â€” Pooled Dataset
  3. ViT-Tiny â€” Custom Dataset
  4. ViT-Tiny â€” Pooled Dataset

âœ” Flask Web App : Upload an image â†’ Model predicts its class + attributes.

âœ” Inference Notebooks : Simple .ipynb notebooks for loading .pth weights and running predictions.

ğŸ§  Model Training Details : 

- Framework: PyTorch + timm
- Optimizer: AdamW
- Loss: Cross-Entropy
- Scheduler: Cosine Annealing
- Batch Size: 32
- Epochs: ~20â€“30
- Augmentations:
  1. Random Resized Crop
  2. Color Jitter
  3. Horizontal Flip
- Identical hyperparameters were used across both datasets to ensure fair comparison.
  
ğŸ“ˆ Evaluation Metrics

The following metrics were used:

      Metric	                 Why
1. Accuracy	            Overall correctness
2. Precision/Recall	    Understand FP/FN behavior
3. Macro F1	            Equal weight to all classes
4. Micro F1	            Weighted by class frequency
5. Confusion Matrix	    Shows misclassified classes

   
ğŸ” Key Results Summary

DeiT-Tiny
- Better performance on small dataset
- Stable training due to distillation
- Higher F1 scores on custom dataset

ViT-Tiny
- Strong improvement on pooled dataset
- Requires more data compared to DeiT-Tiny
- Better generalization with diverse samples

Common Challenges
- Visually similar classes (e.g., mug vs jug)
- Lighting variations
- Transparent/glass objects harder to identify
  
ğŸš€ Running the Web App

Install dependencies:
pip install -r requirements.txt

Start the Flask app:
python web_app/app.py

Then open the local URL shown in terminal:
http://127.0.0.1:5000/
Upload an image â†’ Model predicts class and attributes.

ğŸ”¬ Running Inference Manually
Use any of the notebooks:

inference_notebooks/deit_tiny_inference.ipynb
inference_notebooks/vit_tiny_inference.ipynb

Each notebook includes:
- Code to load the architecture
- Load desired .pth weight
- Preprocess input
- Output prediction

ğŸŒ Applications

1. Everyday object recognition
2. E-commerce auto-tagging
3. Smart inventory classification
4. Visual assistance tools
5. Lightweight edge-device deployment

ğŸ”® Future Improvements

- Add CLIP-based multimodal retrieval
- Integrate attribute prediction
- Improve dataset diversity
- Convert model to ONNX / TFLite for mobile
- Add visualization dashboard
