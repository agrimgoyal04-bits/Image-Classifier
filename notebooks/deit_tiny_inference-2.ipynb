{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b5bdd86",
   "metadata": {},
   "source": [
    "# DeiT-Tiny Inference & Evaluation\n",
    "\n",
    "Inference and evaluation notebook for a fine-tuned `deit_tiny_patch16_224` model.\n",
    "\n",
    "This notebook:\n",
    "- Loads the multi-task ViT model\n",
    "- Runs inference on a chosen split (`train`, `val`, or `test` if available)\n",
    "- Computes the following metrics for the main *class* prediction task:\n",
    "  - Accuracy\n",
    "  - Macro Precision / Recall / F1\n",
    "  - Micro Precision / Recall / F1\n",
    "  - Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137ffd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Paths (adjust if your project layout is different)\n",
    "DATA_ROOT = \"data\"  # folder containing images/, labels.csv, attributes.yaml\n",
    "OUTPUT_DIR = \"outputs\"\n",
    "CHECKPOINT_PATH = os.path.join(OUTPUT_DIR, \"best_model.pth\")  # fine-tuned weights\n",
    "\n",
    "# Data / evaluation settings\n",
    "SPLIT = \"val\"              # typically \"val\" or \"test\"\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "import torch\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3fc345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    confusion_matrix,\n",
    ")\n",
    "\n",
    "from dataset import EverydayObjectsDataset\n",
    "from model import MultiTaskViT\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9705d5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train split once to infer number of classes and class mapping\n",
    "IMG_SIZE = 224  # must match training\n",
    "\n",
    "train_ds = EverydayObjectsDataset(root=DATA_ROOT, split=\"train\", img_size=IMG_SIZE)\n",
    "num_classes = len(train_ds.class_to_idx)\n",
    "print(\"Num classes:\", num_classes)\n",
    "\n",
    "# Build index -> class name mapping\n",
    "idx_to_class = {idx: name for name, idx in train_ds.class_to_idx.items()}\n",
    "class_names = [idx_to_class[i] for i in range(len(idx_to_class))]\n",
    "print(\"Classes:\", class_names)\n",
    "\n",
    "# Attribute schema (used to construct the model)\n",
    "ATTR_YAML_NAME = \"attributes.yaml\"\n",
    "with open(os.path.join(DATA_ROOT, ATTR_YAML_NAME)) as f:\n",
    "    attr_schema = yaml.safe_load(f)\n",
    "print(\"Attributes:\", list(attr_schema.keys()))\n",
    "\n",
    "# Evaluation dataset / loader\n",
    "eval_ds = EverydayObjectsDataset(root=DATA_ROOT, split=SPLIT, img_size=IMG_SIZE)\n",
    "eval_loader = DataLoader(\n",
    "    eval_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "len(eval_loader), len(eval_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26ee519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model and load checkpoint\n",
    "BACKBONE_NAME = \"deit_tiny_patch16_224\"\n",
    "\n",
    "model = MultiTaskViT(\n",
    "    backbone_name=BACKBONE_NAME,\n",
    "    num_classes=num_classes,\n",
    "    attr_schema=attr_schema,\n",
    "    pretrained=False,  # weights will come from the checkpoint\n",
    ")\n",
    "\n",
    "assert os.path.exists(CHECKPOINT_PATH), f\"Checkpoint not found: {CHECKPOINT_PATH}\"\n",
    "\n",
    "checkpoint = torch.load(CHECKPOINT_PATH, map_location=\"cpu\")\n",
    "# Support either plain state_dict or dict with 'model' key\n",
    "state_dict = checkpoint.get(\"model\", checkpoint)\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "print(\"Loaded model from:\", CHECKPOINT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8844f1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference and collect predictions for the main class task\n",
    "all_true = []\n",
    "all_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in eval_loader:\n",
    "        (\n",
    "            imgs,\n",
    "            y_class,\n",
    "            y_color,\n",
    "            y_material,\n",
    "            y_condition,\n",
    "            y_size,\n",
    "            meta,\n",
    "        ) = batch\n",
    "        \n",
    "        imgs = imgs.to(DEVICE)\n",
    "        y_class = y_class.to(DEVICE)\n",
    "        \n",
    "        feats, class_logits, attr_logits = model(imgs)\n",
    "        preds = class_logits.argmax(dim=1)\n",
    "        \n",
    "        all_true.extend(y_class.cpu().numpy())\n",
    "        all_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "all_true = np.array(all_true)\n",
    "all_pred = np.array(all_pred)\n",
    "\n",
    "print(\"Number of samples:\", len(all_true))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5718d3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "acc = accuracy_score(all_true, all_pred)\n",
    "\n",
    "prec_macro, rec_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "    all_true, all_pred, average=\"macro\", zero_division=0\n",
    ")\n",
    "prec_micro, rec_micro, f1_micro, _ = precision_recall_fscore_support(\n",
    "    all_true, all_pred, average=\"micro\", zero_division=0\n",
    ")\n",
    "\n",
    "cm = confusion_matrix(all_true, all_pred)\n",
    "\n",
    "print(f\"Accuracy:       {acc:.4f}\")\n",
    "print(f\"Macro  P/R/F1:  {prec_macro:.4f} / {rec_macro:.4f} / {f1_macro:.4f}\")\n",
    "print(f\"Micro  P/R/F1:  {prec_micro:.4f} / {rec_micro:.4f} / {f1_micro:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca48b58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "im = ax.imshow(cm)\n",
    "\n",
    "ax.set_xticks(range(len(class_names)))\n",
    "ax.set_yticks(range(len(class_names)))\n",
    "ax.set_xticklabels(class_names, rotation=45, ha=\"right\")\n",
    "ax.set_yticklabels(class_names)\n",
    "\n",
    "ax.set_xlabel(\"Predicted label\")\n",
    "ax.set_ylabel(\"True label\")\n",
    "ax.set_title(\"Confusion Matrix\")\n",
    "\n",
    "# Show values in cells\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(j, i, cm[i, j], ha=\"center\", va=\"center\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
